# Visualizando o Backward Pass e o Fluxo de Gradiente com Autograd
Nesse laborat√≥rio veremos como as redes neurais aprendem atrav√©s do motor de diferencia√ß√£o autom√°tica do PyTorch (autograd).

O notebook inclui desde a configura√ß√£o do ambiente e defini√ß√£o de uma arquitetura simples (`SimpleNN`), at√© √† execu√ß√£o do *Forward Pass* para c√°lculo da perda inicial e do *Backward Pass* para gera√ß√£o de gradientes.

O processo termina com a atualiza√ß√£o dos pesos e vieses atrav√©s de um otimizador (SGD), permitindo visualizar, tanto numericamente quanto graficamente, como a rede ajusta os seus par√¢metros internos para reduzir o erro e aproximar as suas previs√µes dos dados reais.

Clique em uma dos links abaixo para abrir o notebook:
<table >
  <td>
    <a href="https://colab
    - research.google.com/github/fabiobento/dnn-course-2026-1/blob/main/aulas/dl-pytorch/intro-dl/docs/autograd.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
  </td>
  <td>
    <a target="_blank" href="https://kaggle.com/kernels/welcome?src=https://github.com/fabiobento/dnn-course-2026-1/blob/main/aulas/dl-pytorch/intro-dl/docs/autograd.ipynb"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a>
  </td>
</table>

||
|---|
|Item anterior **[‚¨ÖÔ∏èConstruindo uma Rede Neural e Visualizando o Forward Pass?](./first-nn.md)**|
|Pr√≥ximo item **[‚û°Ô∏èDesenvolva e visualize um perceptron do zero](./perceptron.md)** |
||
|P√°gina inicial do curso **[üè†√çndice do Curso de Deep Learning com PyTorch](../README.md)**|