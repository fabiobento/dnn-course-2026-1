# Visualizando o Backward Pass e o Fluxo de Gradiente com Autograd
Nesse laboratório veremos como as redes neurais aprendem através do motor de diferenciação automática do PyTorch (autograd).

O notebook inclui desde a configuração do ambiente e definição de uma arquitetura simples (`SimpleNN`), até à execução do *Forward Pass* para cálculo da perda inicial e do *Backward Pass* para geração de gradientes.

O processo termina com a atualização dos pesos e vieses através de um otimizador (SGD), permitindo visualizar, tanto numericamente quanto graficamente, como a rede ajusta os seus parâmetros internos para reduzir o erro e aproximar as suas previsões dos dados reais.

Clique em uma dos links abaixo para abrir o notebook:
<table >
  <td>
    <a href="https://colab.research.google.com/github/fabiobento/dnn-course-2026-1/blob/main/aulas/dl-pytorch/intro-dl/docs/autograd.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
  </td>
  <td>
    <a target="_blank" href="https://kaggle.com/kernels/welcome?src=https://github.com/fabiobento/dnn-course-2026-1/blob/main/aulas/dl-pytorch/intro-dl/docs/autograd.ipynb"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a>
  </td>
</table>

||
|---|
|Próximo item **[➡️Laboratório - Desenvolva e visualize um perceptron do zero](./lab-perceptron.md)** |
|Item anterior **[⬅️Construindo uma Rede Neural e Visualizando o Forward Pass?](./first-nn.md)**|
||
|Página inicial do curso **[⬆️Introdução ao Deep Learning com PyTorch](./intro-dl.md)**|
