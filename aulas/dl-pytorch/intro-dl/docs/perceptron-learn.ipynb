{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idIYCKZEtfCu"
      },
      "source": [
        "# A regra de aprendizagem do perceptron e as atualizações de peso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table>\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/fabiobento/dnn-course-2026-1/blob/main/aulas/dl-pytorch/intro-dl/docs/perceptron-learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/fabiobento/dnn-course-2026-1/blob/main/aulas/dl-pytorch/intro-dl/docs/perceptron-learn.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Este código demonstra o cerne da Regra de Aprendizagem do Perceptron usando dados para uma porta AND.\n",
        "    - Começamos com pesos e viés definidos como zero.\n",
        "    - Em seguida, o código percorre cada entrada x e sua saída alvo das matrizes X e y.\n",
        "    - Para cada entrada, ele calcula uma saída linear e, em seguida, aplica uma função de passo simples para obter uma previsão (0 ou 1).\n",
        "    - O erro é encontrado subtraindo a previsão do alvo.\n",
        "    - Fundamentalmente, somente se houver um erro, os pesos e o viés são ajustados usando a fórmula da Regra de Aprendizagem do Perceptron. Esta é a etapa central da aprendizagem.\n",
        "    - Após uma passagem completa (época) pelos dados, os pesos e o viés finais são impressos.\n",
        "    - Por fim, testamos o perceptron com os pesos e o viés aprendidos para ver suas previsões."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Weights: [0. 0.] Bias: 0.0\n",
            "\n",
            "--- Training (One Epoch) ---\n",
            "  Input: [0 0], Target: 0, Predicted: 1, Error: -1 -> UPDATED!\n",
            "  Input: [0 1], Target: 0, Predicted: 0, Error: 0 -> No update needed.\n",
            "  Input: [1 0], Target: 0, Predicted: 0, Error: 0 -> No update needed.\n",
            "  Input: [1 1], Target: 1, Predicted: 0, Error: 1 -> UPDATED!\n",
            "\n",
            "Final Weights after one epoch: [0.1 0.1] Bias: 0.0\n",
            "\n",
            "--- Testing final perceptron ---\n",
            "  Input: [0 0], Predicted: 1, Actual: 0\n",
            "  Input: [0 1], Predicted: 1, Actual: 0\n",
            "  Input: [1 0], Predicted: 1, Actual: 0\n",
            "  Input: [1 1], Predicted: 1, Actual: 1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# --- 1. Define Data (AND gate logic) ---\n",
        "# Each row is an input [x1, x2]\n",
        "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
        "# Corresponding target outputs for the AND gate\n",
        "y = np.array([0, 0, 0, 1])\n",
        "# --- 2. Perceptron Parameters ---\n",
        "weights = np.array([0.0, 0.0]) # Initial weights\n",
        "bias = 0.0  # Initial bias\n",
        "learning_rate = 0.1 # How big the steps are\n",
        "print(\"Initial Weights:\", weights, \"Bias:\", bias)\n",
        "# --- 3. Training Loop (Single Epoch for demonstration) ---\n",
        "print(\"\\n--- Training (One Epoch) ---\")\n",
        "for i, x in enumerate(X):\n",
        "    target = y[i]\n",
        "    # Calculate the perceptron's output\n",
        "    # # Step 1: Weighted sum\n",
        "    linear_output = np.dot(x, weights) + bias\n",
        "    # Step 2: Activation (step function)\n",
        "    output = 1 if linear_output >= 0 else 0\n",
        "    # Calculate the error\n",
        "    error = target - output\n",
        "    # Update weights and bias ONLY if there's an error\n",
        "    if error != 0:\n",
        "        # Update each weight: weight = weight + learning_rate * error * input\n",
        "        weights += learning_rate * error * x\n",
        "        # Update bias: bias = bias + learning_rate * error\n",
        "        bias += learning_rate * error\n",
        "        print(f\"  Input: {x}, Target: {target}, Predicted: {output}, Error: {error} -> UPDATED!\")\n",
        "    else:\n",
        "        print(f\"  Input: {x}, Target: {target}, Predicted: {output}, Error: {error} -> No update needed.\")\n",
        "print(\"\\nFinal Weights after one epoch:\", weights, \"Bias:\", bias)\n",
        "# --- 4. Test (Simple Prediction) ---\n",
        "print(\"\\n--- Testing final perceptron ---\")\n",
        "for i, x in enumerate(X):\n",
        "    linear_output = np.dot(x, weights) + bias\n",
        "    prediction = 1 if linear_output >= 0 else 0\n",
        "    print(f\"  Input: {x}, Predicted: {prediction}, Actual: {y[i]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
